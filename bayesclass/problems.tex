\documentclass[10pt,a4paper]{article}
\usepackage[latin1]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{fullpage}
\author{Patrick Lam}

\parindent=0in

\begin{document}

\begin{center}
\begin{Large}Week 1 Problems\end{Large}
\end{center}
\bigskip
\begin{enumerate}

\item An urn contains 10 red balls and 15 white balls. You pick two balls at random without replacement.
\begin{enumerate}
\item[a)] What is the probability that the first ball is red?
\item[b)] What is the probability that the second ball is red?
\item[c)] What is the probability that both balls are white?
\item[d)] What is the probability that the second ball is red given that the first ball is white?
\item[e)] What is the probability that the first ball is red given that the second ball is white?
\end{enumerate}

\bigskip

\item (From Gelman 3.7) A student sits on a street corner for an hour and records the number of bicycles $b$ and the number of other vehicles $v$ that go by.  Two models are considered:
\begin{itemize}
\item The outcomes $b$ and $v$ have independent Poisson distributions, with unknown means $\theta_b$ and $\theta_v$.
\item The outcome $b$ has a binomial distribution, with unknown probability $p$ and sample size $b + v$.
\end{itemize}
Show that the two models have the same likelihood if we define $p = \frac{\theta_b}{\theta_b + \theta_v}$. \\

Hints:
\begin{itemize}
\item Find the conditional distribution of $b$ conditioning on information you know.
\item If $X \sim \mathrm{Poisson}(\theta_1)$ and $Y \sim \mathrm{Poisson}(\theta_2)$, then $X+Y \sim \mathrm{Poisson}(\theta_1 + \theta_2)$.
\end{itemize}

\bigskip 

\item Let $X \sim$ Uniform(1,4). Use calculus to find $E(X)$ and Var($X$).

\bigskip 

\item 
\begin{enumerate}
\item[a)]
Suppose you have $n$ independent observations $X_i$ from an exponential distribution where 
\begin{eqnarray*}
p(X_i | \lambda) = \lambda e^{-\lambda x_i}
\end{eqnarray*}
Analytically find the maximum likelihood estimate of $\lambda$.
\medskip
\item[b)] Now reparameterize the distribution for $X_i$ in terms of $\tau$ where 
\begin{eqnarray*}
\tau = \frac{1}{\lambda}
\end{eqnarray*}
Find the MLE for $\tau$.
\end{enumerate}

\bigskip

\item Suppose that $X$ follows a Gamma($\alpha, \beta$) distribution.  Show that $\frac{1}{X}$ follows an Inv-Gamma($\alpha$, $\beta$) distribution.

\begin{itemize}
\item Gamma PDF: $p(y) = \frac{\beta^{\alpha}}{\Gamma(\alpha)} y^{\alpha-1} e^{-\beta y}$
\item Inverse Gamma PDF: $p(y) = \frac{\beta^{\alpha}}{\Gamma(\alpha)} y^{-(\alpha+1)} e^{-\beta/y}$
\item Change of Variables formula: Let $Y = g(X)$ and $X = g^{-1}(Y)$.  Then 
\begin{eqnarray*}
p_Y(y) = p_X(g^{-1}(y)) \left \vert \frac{d}{dy} g^{-1}(y) \right \vert
\end{eqnarray*}
\end{itemize}

\bigskip


\end{enumerate}

\end{document}





\bigskip

\item (Adapted from Gelman 2.10) Suppose there are $N$ cable cars in San Francisco, numbered sequentially from 1 to $N$.  You see a cable car at random; it is numbered 203.  You wish to estimate $N$. \\

Assume your prior distribution on $N$ is geometric with mean 100; that is,
\begin{eqnarray*}
p(N) = (1/100)(99/100)^{N-1}, \; \mathrm{for} \; N = 1,2,\dots
\end{eqnarray*}

\begin{itemize}
\item[a)] What is your posterior distribution for $N$ up to a constant of proportionality?   \\
\item[b)] Find the posterior mean and standard deviation by approximating the normalizing constant in R (without simulating). \\
\item[c)] Now find the posterior mean and standard deviation by simulation in R.  Are your answers in b) and c) similar?
\end{itemize}


\bigskip

\item Suppose $Y$ is distributed Poisson($\lambda$) with a Gamma($\alpha, \beta$) prior on $\lambda$.  Derive the posterior for $\lambda$, $p(\lambda | y)$.  What is this distribution?
