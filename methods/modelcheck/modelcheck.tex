\documentclass{beamer}


\usetheme{default}
\usepackage{subfigure}
\usepackage{amsmath}
\usepackage{Sweave}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{multicol}
\usepackage[all]{xy}
\usepackage{bm}


\author{Patrick Lam}
\title{Model Checking}
\date{}
%\date{December 1, 2008}

\begin{document}

\newcommand{\red}{\textcolor{red}}
\newcommand{\blue}{\textcolor{blue}}
\newcommand{\cyan}{\textcolor{cyan}}
\newcommand{\purple}{\textcolor{purple}}
\newcommand{\brown}{\textcolor{brown}}

\frame{\titlepage}

\begin{frame}
\frametitle{Outline}
\tableofcontents
\end{frame}

\section{Posterior Predictive Distribution}

\begin{frame}
\frametitle{Outline}
\tableofcontents[currentsection]
\end{frame}

\begin{frame}
\frametitle{Prediction}
\pause
Once we have a model and generated draws from our \blue{posterior}
distribution, we may want to predict future data points.  \\
\pause
\bigskip
We may want to make predictions in order to:
\pause
\begin{enumerate}
\item Predict how a system would behave in the future (substantive implications)
\pause
\item Assess model accuracy (modeling implications)
\end{enumerate}
\pause
\bigskip
Through simulation, we can get a \textbf{posterior predictive distribution}.
\end{frame}

\begin{frame}
\frametitle{Posterior Predictive Distribution}
\pause
Predicted distribution of some future data point(s) $y^{\mathrm{rep}}$ after
having seen the data $y$.
\pause
\begin{eqnarray*}
p(y^{\mathrm{rep}} | y) &=& \int p(y^{\mathrm{rep}}, \theta | y) d\theta \\
&=& \int p(y^{\mathrm{rep}} | \theta, y) \textcolor{blue}{p(\theta | y)} d\theta
\end{eqnarray*}
\pause
If we assume $y \bot y^{\mathrm{rep}} | \theta$, then
\pause
\begin{eqnarray*}
p(y^{\mathrm{rep}} | y) &=&  \int p(y^{\mathrm{rep}} | y) \textcolor{blue}{p(\theta | y)} d\theta
\end{eqnarray*}
\pause
If $y$ is a vector of $n$ observations, then $y^{\mathrm{rep}}$ is
also a vector of length $n$ with covariates set at the observed (model
checking) or hypothetical values (prediction) \pause and
$p(y^{\mathrm{rep}} | y)$ can be thought of as an $n$-variate distribution.
\end{frame}

\begin{frame}[fragile]
We can simulate the \textcolor{blue}{posterior} predictive distribution.
\pause
\bigskip
\begin{enumerate}
\item Sample $m$ values of $\theta$ from our \textcolor{blue}{posterior}.
\pause
\item For each \textcolor{blue}{posterior} draw, sample a value (vector) of
$y^{\mathrm{rep}}$ from our likelihood $p(y^{\mathrm{rep}} | \theta)$.
\end{enumerate}
\bigskip
\pause
The $m$ values (vectors) of $y^{\mathrm{rep}}$ represent draws from
the \blue{posterior} predictive distribution $p(y^{\mathrm{rep}} | y)$.\\
\pause
\bigskip
We can use the posterior predictive distribution to predict the future
or assess model accuracy with posterior predictive checks.
\end{frame}

\section{Posterior Predictive Checks}

\begin{frame}
\frametitle{Outline}
\tableofcontents[currentsection]
\end{frame}

\begin{frame}
Much of what we have done so far is based on
a model that we specify, which may or may not be accurate. \\
\bigskip
\pause
Specifically, we make many assumptions with our model which may or may
not be accurate (for example, independence across observations). \\
\bigskip
\pause
We can attempt to check specific model assumptions with \textbf{posterior predictive checks}.
\end{frame}

\begin{frame}
\frametitle{Posterior Predictive Checks}
\pause
To conduct a posterior predictive check, do the following:
\medskip
\pause
\begin{enumerate}
\item Come up with a test statistic $T$ that has power to diagnose
violations of whatever assumption you are testing. 
\medskip
\pause
\item Calculate $T$ for the observed data $y$: \pause $T(y)$
\medskip
\pause
\item Calculate $T$ for each $y^{\mathrm{rep}}$ draw from the
posterior predictive distribution: \pause $T(y^{\mathrm{rep}} | y)$
\medskip
\pause
\item Calculate the fraction of times $T(y^{\mathrm{rep}} | y) >
T(y)$.  \pause  This is an estimate of the \textit{posterior
predictive $p$-value}. 
\end{enumerate}
\end{frame}

\begin{frame}
The idea is that if our data violates one of our model assumptions,
then our observed test statistic $T(y)$ should be significantly
different than our model predicted test statistics $T(y^{\mathrm{rep}} | y)$.\\
\bigskip
\pause
If our posterior predictive $p$-value is close to 0 or 1 (say 0.05 or
0.95), then it suggests that our observed data has an extreme test
statistic and that something in our model may be inadequate.
\end{frame}

\begin{frame}
\frametitle{Possible Problems with Posterior Predictive Checks}
\pause
\begin{itemize}
\item Choice of test statistic is very important.
\pause
\begin{itemize}
\item Test statistic must be meaningful and pertinent to the
assumption you want to test.
\pause
\item Test statistics often have low power (inability to find problems
when problems exist)
\pause
\item Test statistics should be not based on aspects of the data that
are being explicit modeled (for example, the mean of $y$ in a linear model).
\end{itemize}
\pause
\medskip
\item If the model passes posterior predictive check, it does not
necessarily mean there are no problems with the model.
\begin{itemize}
\pause
\item Test statistic may have low power.
\pause
\item May be testing the wrong assumption.
\end{itemize}
\pause
\medskip
\item It is not always clear how to correct the incorrect model assumptions.
\end{itemize}
\end{frame}

\subsection{An Example}

\begin{frame}
\frametitle{Outline}
\tableofcontents[currentsubsection]
\end{frame}

\begin{frame}[fragile]
\frametitle{Running Example}
\pause
Time-series cross-sectional dataset on civil war onset from Fearon and Laitin.
\pause
\tiny
\medskip
\begin{Schunk}
\begin{Sinput}
> data <- read.table("FLdata.txt")
\end{Sinput}
\end{Schunk}
\normalsize
\pause
\bigskip
Dependent variable: binary variable on civil war onset\\
\pause
\bigskip
Independent variables: the normal set of independent variables
predicting civil wars \\
\pause
\bigskip
Model: Bayesian logistic regression with binomial likelihood and
multivariate Normal priors (using {\tt MCMCpack})
\pause
\tiny
\medskip
\begin{Schunk}
\begin{Sinput}
> library(MCMCpack)
\end{Sinput}
\end{Schunk}
\normalsize
\end{frame}

\begin{frame}[fragile]
\frametitle{Posterior Predictive Distribution}
\pause
\begin{enumerate}
\item Create model matrix of covariates $X$.
\tiny 
\medskip
\pause
\pause
\normalsize
\medskip
\item Get linear predictors by multiplying $X$ and our $m$ draws from
the posterior.
\tiny 
\medskip
\pause
\pause
\normalsize
\medskip
\item Convert linear predictors into probabilities with the inverse logit function.
\tiny 
\medskip
\pause
\pause
\normalsize
\medskip
\item Draw $m$ samples of $y^{\mathrm{rep}}$ from the binomial likelihood.
\tiny 
\medskip
\pause
\pause
\normalsize
\end{enumerate}
The resulting posterior predictive distribution is an $n \times m$
matrix.
\end{frame}

\begin{frame}
\frametitle{A Bad Test Statistic}
\pause
Let $T$ = the fraction of y's that take on the value of 1 \\
\bigskip
\pause
What's wrong with this test statistic?
\pause
\medskip
\begin{itemize}
\item Unclear what assumption are we testing.
\pause
\item The fraction of 1s is explicitly being modeled in the logit model.
\pause
\begin{itemize}
\item The test will never show anything is wrong regardless of how bad
our model is.
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{A Better Test Statistic}
\pause
Assumption: No clustering within years\\
\pause
\bigskip
Test Statistic: $T$ = the variance of the number of 1s in each year
\end{frame}

\begin{frame}[fragile]
\begin{enumerate}
\item Come up with a test statistic $T$ that has power to diagnose
violations of whatever assumption you are testing: \pause  $T$ = the variance of the number of 1s in each year
\medskip
\pause
\item Calculate $T$ for the observed data $y$: \pause $T(y)$
\medskip
\pause
\tiny
\pause
\medskip
\normalsize 
\item Calculate $T$ for each $y^{\mathrm{rep}}$ draw from the
posterior predictive distribution: \pause $T(y^{\mathrm{rep}} | y)$
\medskip
\pause
\tiny
\end{enumerate}
\normalsize 
\end{frame}

\begin{frame}[fragile]
\begin{enumerate}
\item[4.] Calculate the fraction of times $T(y^{\mathrm{rep}} | y) >
T(y)$.  \pause  This is an estimate of the \textit{posterior
predictive $p$-value}. 
\medskip
\pause
\tiny
\end{enumerate}
\begin{figure}[!htp]
\includegraphics[width = 1.5in, height=1.5in]{modelcheck-yearhist.pdf}
\end{figure}
\normalsize
\pause
Does this mean our assumption is correct?  \pause Not necessarily (low
power?)
\end{frame}

\section{Bayes Factor}

\begin{frame}
\frametitle{Outline}
\tableofcontents[currentsection]
\end{frame}

\begin{frame}
\frametitle{Bayes Factor}
\pause
Suppose we have two competing models/hypotheses: \pause $M_1$ and $M_2$ \\
\pause
\bigskip
\textit{Assuming that either $M_1$ or $M_2$ is true}, we would like to know what the data $\bm{y}$ tell us about the probabilities of either model being true. \\
\pause
\bigskip
That is, we would like to know $p(M_1 | \bm{y})$ and $p(M_2 | \bm{y})$. \\
\bigskip
\pause 
We can then compare $p(M_1 | \bm{y})$ and $p(M_2 | \bm{y})$ to see which model fits the data better. \\
\pause
\bigskip
This is known as the Bayes factor approach and it is the Bayesian alternative to hypothesis testing in classical statistics.
\end{frame}

\begin{frame}
Note that $p(M_1 | \bm{y})$ and $p(M_2 | \bm{y})$ are \cyan{posteriors} of $M_1$ and $M_2$ respectively.\\
\pause
\bigskip
In comparing the two models, we want to look at the \cyan{posterior} odds in favor of one model (say $M_1$):
\pause
\begin{eqnarray*}
\frac{\cyan{p(M_1 | \bm{y})}}{\cyan{p(M_2 | \bm{y})}} &=& \frac{\frac{p(\bm{y} | M_1) p(M_1)}{p(\bm{y})}}{\frac{p(\bm{y} | M_2) p(M_2)}{p(\bm{y})}} \\
\pause
&=& \frac{p(\bm{y} | M_1)}{p(\bm{y} | M_2)}  \frac{\purple{p(M_1)}}{\purple{p(M_2)}}
\end{eqnarray*}
\pause
The term $\frac{p(\bm{y} | M_1)}{p(\bm{y} | M_2)}$ is known as the \textbf{Bayes factor}.\\
\pause
\begin{eqnarray*}
\mathrm{\cyan{posterior \; odds} = Bayes \; factor \times \purple{prior \; odds}}
\end{eqnarray*} 
\end{frame}

\begin{frame}
If both models are equally likely \textit{a priori} ($\purple{p(M_1)} = \purple{p(M_2)}$), then the \cyan{posterior} odds equal the Bayes factor.  
\pause
\begin{eqnarray*}
\frac{\cyan{p(M_1 | \bm{y})}}{\cyan{p(M_2 | \bm{y})}} = \frac{p(\bm{y} | M_1)}{p(\bm{y} | M_2)} 
\end{eqnarray*}
\pause
Thus, the Bayes factor is a measure of how much the data supports one model relative to the other. \\
\pause
\bigskip
The individual terms that make up the Bayes factor, $p(\bm{y} | M_1)$ and $p(\bm{y} | M_2)$, are known as \textbf{marginal likelihoods}
\end{frame}

\begin{frame}
\frametitle{Marginal Likelihood}
\pause
The marginal likelihood for model $M_k$ is
\begin{eqnarray*}
p(\bm{y} | M_k) = \int_{\bm{\Theta}_k} p(\bm{y} | \bm{\theta}_k, M_k) \red{p(\bm{\theta}_k | M_k)} d\bm{\theta}_k
\end{eqnarray*}
\pause
where $\bm{\theta}_k$ are the model parameters for model $M_k$. \\
\pause
\bigskip
Thus, the marginal likelihood is "marginal" because it is the likelihood of $\bm{y}$ under $M_k$ \textit{averaged} over the model parameters $\bm{\theta}_k$.\\
\pause
\bigskip
Note that \red{$p(\bm{\theta}_k | M_k)$} is just the \red{prior} for $\bm{\theta}$ under $M_k$. \\
\pause
\bigskip
The marginal likelihood can be interpreted as 
\pause
\begin{itemize}
\item the normalizing constant of the \blue{posterior $p(\bm{\theta} | \bm{y})$} given $M_k$
\pause
\item the expected value of the likelihood function taken over the \red{prior} density
\end{itemize}
\end{frame}

\begin{frame}
Note that the marginal likelihood is just 
\begin{eqnarray*}
p(\bm{y}) = \int_{\bm{\Theta}} p(\bm{y} | \bm{\theta}) \red{p(\bm{\theta})} d\bm{\theta}
\end{eqnarray*}
conditioned on $M_k$. \\
\pause
\bigskip
Also recall Bayes rule:
\begin{eqnarray*}
\blue{p(\bm{\theta} | \bm{y})} = \frac{p(\bm{y} | \bm{\theta}) \red{p(\bm{\theta})}}{p(\bm{y})}
\end{eqnarray*}
\pause
This means that for model $M_k$, the marginal likelihood is just the normalizing constant for the \blue{posterior} of $\bm{\theta}$! \\
\pause
\bigskip
Therefore, except in simple cases (such as conjugacy), the marginal likelihood usually has to be approximated.
\end{frame}

\begin{frame}
With the (approximate) marginal likelihoods, the Bayes factor can be calculated:
\pause
\begin{eqnarray*}
\frac{p(\bm{y} | M_1)}{p(\bm{y} | M_2)} = \frac{\int_{\bm{\Theta}_1} p(\bm{y} | \bm{\theta}_1, M_1) p(\bm{\theta}_1 | M_1) d\bm{\theta}_1}{\int_{\bm{\Theta}_2} p(\bm{y} | \bm{\theta}_2, M_2) p(\bm{\theta}_2 | M_2) d\bm{\theta}_2} \\
\end{eqnarray*}
\pause
\bigskip
Interpretation of the Bayes factor is somewhat arbitrary. \\
\pause 
Generally speaking, for the Bayes factor of $M_1$ over $M_2$, a Bayes factor 
\begin{itemize}
\item less than $\frac{1}{100}$ leads us to reject $M_1$ 
\pause
\item greater than 1 leads us to accept $M_1$
\end{itemize}
\end{frame}

\begin{frame}
One can also rely on tables such as the one given by Jeffreys for the Bayes factor of $M_1$ over $M_2$:
\pause
\bigskip
\begin{table}
\begin{center}
\begin{tabular}{cc}
\hline
Bayes Factor & Strength of Evidence for $M_1$\\
\hline
$< 1$ & Negative (supports $M_2$)\\
1 to 3 & Barely Worth Mentioning \\
3 to 10 & Substantial \\
10 to 30 & Strong \\
30 to 100 & Very Strong \\ 
> 100 & Decisive \\
\hline
\end{tabular}
\end{center}
\end{table}
\end{frame}

\end{document}
